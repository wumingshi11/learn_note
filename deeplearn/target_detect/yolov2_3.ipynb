{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yolov2创新点\n",
    "1. 使用预定义框进行训练，预定义框基于k聚类生成，模型产出是基于预定义框的偏移量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 生成预定义框\n",
    "    def generate_anchors(self, fmp_size):\n",
    "        \"\"\" \n",
    "            用于生成G矩阵，其中每个元素都是特征图上的像素坐标和先验框的尺寸。\n",
    "        \"\"\"\n",
    "        fmp_h, fmp_w = fmp_size\n",
    "\n",
    "        # generate grid cells\n",
    "        anchor_y, anchor_x = torch.meshgrid([torch.arange(fmp_h), torch.arange(fmp_w)])\n",
    "        anchor_xy = torch.stack([anchor_x, anchor_y], dim=-1).float().view(-1, 2)\n",
    "        # [HW, 2] -> [HW, A, 2] -> [M, 2]\n",
    "        anchor_xy = anchor_xy.unsqueeze(1).repeat(1, self.num_anchors, 1)\n",
    "        anchor_xy = anchor_xy.view(-1, 2).to(self.device)\n",
    "\n",
    "        # [A, 2] -> [1, A, 2] -> [HW, A, 2] -> [M, 2]\n",
    "        anchor_wh = self.anchor_size.unsqueeze(0).repeat(fmp_h*fmp_w, 1, 1)\n",
    "        anchor_wh = anchor_wh.view(-1, 2).to(self.device)\n",
    "\n",
    "        anchors = torch.cat([anchor_xy, anchor_wh], dim=-1)\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    # 预测框解码，基于生成的预定义框\n",
    "    def decode_boxes(self, anchors, pred_reg):\n",
    "        \"\"\"\n",
    "            将YOLO预测的 (tx, ty)、(tw, th) 转换为bbox的左上角坐标 (x1, y1) 和右下角坐标 (x2, y2)。\n",
    "            输入:\n",
    "                pred_reg: (torch.Tensor) -> [B, HxWxA, 4] or [HxWxA, 4]，网络预测的txtytwth\n",
    "                fmp_size: (List[int, int])，包含输出特征图的宽度和高度两个参数\n",
    "            输出:\n",
    "                pred_box: (torch.Tensor) -> [B, HxWxA, 4] or [HxWxA, 4]，解算出的边界框坐标\n",
    "        \"\"\"\n",
    "        # 计算预测边界框的中心点坐标和宽高\n",
    "        pred_ctr = (torch.sigmoid(pred_reg[..., :2]) + anchors[..., :2]) * self.stride\n",
    "        pred_wh = torch.exp(pred_reg[..., 2:]) * anchors[..., 2:]\n",
    "\n",
    "        # 将所有bbox的中心带你坐标和宽高换算成x1y1x2y2形式\n",
    "        pred_x1y1 = pred_ctr - pred_wh * 0.5\n",
    "        pred_x2y2 = pred_ctr + pred_wh * 0.5\n",
    "        pred_box = torch.cat([pred_x1y1, pred_x2y2], dim=-1)\n",
    "\n",
    "        return pred_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolov3 主要创新点\n",
    "1. 特征金字塔\n",
    "   - 主干网络输出3个不同大小分辨率的特征图\n",
    "   - 对第三个特征图进行SPP操作\n",
    "   - 将第三个特征图和第二个特征图上采用，与第一个特征图拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 主干网络输出3个不同分辨率的特征图\n",
    "## DarkNet-Tiny\n",
    "class DarkNetTiny(nn.Module):\n",
    "    def __init__(self, act_type='silu', norm_type='BN'):\n",
    "        super(DarkNetTiny, self).__init__()\n",
    "        self.feat_dims = [64, 128, 256]\n",
    "\n",
    "        # stride = 2\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            Conv(3, 16, k=3, p=1, s=2, act_type=act_type, norm_type=norm_type),\n",
    "            ResBlock(16, 16, nblocks=1, act_type=act_type, norm_type=norm_type)\n",
    "        )\n",
    "        # stride = 4\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            Conv(16, 32, k=3, p=1, s=2, act_type=act_type, norm_type=norm_type),\n",
    "            ResBlock(32, 32, nblocks=1, act_type=act_type, norm_type=norm_type)\n",
    "        )\n",
    "        # stride = 8\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            Conv(32, 64, k=3, p=1, s=2, act_type=act_type, norm_type=norm_type),\n",
    "            ResBlock(64, 64, nblocks=3, act_type=act_type, norm_type=norm_type)\n",
    "        )\n",
    "        # stride = 16\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            Conv(64, 128, k=3, p=1, s=2, act_type=act_type, norm_type=norm_type),\n",
    "            ResBlock(128, 128, nblocks=3, act_type=act_type, norm_type=norm_type)\n",
    "        )\n",
    "        # stride = 32\n",
    "        self.layer_5 = nn.Sequential(\n",
    "            Conv(128, 256, k=3, p=1, s=2, act_type=act_type, norm_type=norm_type),\n",
    "            ResBlock(256, 256, nblocks=2, act_type=act_type, norm_type=norm_type)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.layer_1(x)\n",
    "        c2 = self.layer_2(c1)\n",
    "        c3 = self.layer_3(c2)\n",
    "        c4 = self.layer_4(c3)\n",
    "        c5 = self.layer_5(c4)\n",
    "\n",
    "        outputs = [c3, c4, c5]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Pyramid Pooling， spatial pyramid，通过池化层进行采样，扩大视域，融合不同大小的特征\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"\n",
    "        该代码参考YOLOv5的官方代码实现 https://github.com/ultralytics/yolov5\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, expand_ratio=0.5, pooling_size=5, act_type='lrelu', norm_type='BN'):\n",
    "        super().__init__()\n",
    "        inter_dim = int(in_dim * expand_ratio)\n",
    "        self.out_dim = out_dim\n",
    "        self.cv1 = Conv(in_dim, inter_dim, k=1, act_type=act_type, norm_type=norm_type)\n",
    "        self.cv2 = Conv(inter_dim * 4, out_dim, k=1, act_type=act_type, norm_type=norm_type)\n",
    "        self.m = nn.MaxPool2d(kernel_size=pooling_size, stride=1, padding=pooling_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        y1 = self.m(x)\n",
    "        y2 = self.m(y1)\n",
    "\n",
    "        return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n",
    "\n",
    "\n",
    "def build_neck(cfg, in_dim, out_dim):\n",
    "    model = cfg['neck']\n",
    "    print('==============================')\n",
    "    print('Neck: {}'.format(model))\n",
    "    # build neck\n",
    "    if model == 'sppf':\n",
    "        neck = SPPF(\n",
    "            in_dim=in_dim,\n",
    "            out_dim=out_dim,\n",
    "            expand_ratio=cfg['expand_ratio'], \n",
    "            pooling_size=cfg['pooling_size'],\n",
    "            act_type=cfg['neck_act'],\n",
    "            norm_type=cfg['neck_norm']\n",
    "            )\n",
    "    else:\n",
    "        raise NotImplementedError('Neck {} not implemented.'.format(cfg['neck']))\n",
    "\n",
    "    return neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov3FPN\n",
    "class Yolov3FPN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dims=[256, 512, 1024],\n",
    "                 width=1.0,\n",
    "                 depth=1.0,\n",
    "                 out_dim=None,\n",
    "                 act_type='silu',\n",
    "                 norm_type='BN'):\n",
    "        super(Yolov3FPN, self).__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.out_dim = out_dim\n",
    "        c3, c4, c5 = in_dims\n",
    "\n",
    "        # P5 -> P4 \n",
    "        # top_down_layer 用于融合不同层次图像\n",
    "        # reduce_layer 用于降低通道数\n",
    "        self.top_down_layer_1 = ConvBlocks(c5, int(512*width), act_type=act_type, norm_type=norm_type)\n",
    "        self.reduce_layer_1 = Conv(int(512*width), int(256*width), k=1, act_type=act_type, norm_type=norm_type)\n",
    "        # 上采样保证输出特征图尺寸一致，在计算时进行采样\n",
    "        # P4 -> P3\n",
    "        self.top_down_layer_2 = ConvBlocks(c4 + int(256*width), int(256*width), act_type=act_type, norm_type=norm_type)\n",
    "        self.reduce_layer_2 = Conv(int(256*width), int(128*width), k=1, act_type=act_type, norm_type=norm_type)\n",
    "        # 上采样保证输出特征图尺寸一致，在计算时进行采样\n",
    "        # P3\n",
    "        self.top_down_layer_3 = ConvBlocks(c3 + int(128*width), int(128*width), act_type=act_type, norm_type=norm_type)\n",
    "\n",
    "        # output proj layers\n",
    "        if out_dim is not None:\n",
    "            # output proj layers\n",
    "            self.out_layers = nn.ModuleList([\n",
    "                Conv(in_dim, out_dim, k=1,\n",
    "                        norm_type=norm_type, act_type=act_type)\n",
    "                        for in_dim in [int(128 * width), int(256 * width), int(512 * width)]\n",
    "                        ])\n",
    "            self.out_dim = [out_dim] * 3\n",
    "\n",
    "        else:\n",
    "            self.out_layers = None\n",
    "            self.out_dim = [int(128 * width), int(256 * width), int(512 * width)]\n",
    "\n",
    "\n",
    "    def forward(self, features):\n",
    "        c3, c4, c5 = features\n",
    "        \n",
    "        # p5/32\n",
    "        p5 = self.top_down_layer_1(c5)\n",
    "\n",
    "        # p4/16\n",
    "        p5_up = F.interpolate(self.reduce_layer_1(p5), scale_factor=2.0)\n",
    "        p4 = self.top_down_layer_2(torch.cat([c4, p5_up], dim=1))\n",
    "\n",
    "        # P3/8\n",
    "        p4_up = F.interpolate(self.reduce_layer_2(p4), scale_factor=2.0)\n",
    "        p3 = self.top_down_layer_3(torch.cat([c3, p4_up], dim=1))\n",
    "\n",
    "        out_feats = [p3, p4, p5]\n",
    "\n",
    "        # output proj layers\n",
    "        if self.out_layers is not None:\n",
    "            # output proj layers\n",
    "            out_feats_proj = []\n",
    "            for feat, layer in zip(out_feats, self.out_layers):\n",
    "                out_feats_proj.append(layer(feat))\n",
    "            return out_feats_proj\n",
    "\n",
    "        return out_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
